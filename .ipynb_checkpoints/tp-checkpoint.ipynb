{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda cuda\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device, device.type)\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /home/garoubahiefissa/.conda/envs/chps906/lib/python3.13/site-packages (0.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juliet3.cluster.local\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(4, 4), stride=(4, 4))\n"
     ]
    }
   ],
   "source": [
    "#emulating patch size of 4*4 using CNN \n",
    "\n",
    "height = 32\n",
    "width = 32\n",
    "\n",
    "color_channels = 3\n",
    "\n",
    "patch_size = 4\n",
    "\n",
    "# calculate the number of patches for the image\n",
    "nb_patches = height * width // patch_size**2# 64 here\n",
    "\n",
    "#use 2D conv layer generate patches\n",
    "patchenizer = nn.Conv2d(in_channels=color_channels, out_channels=nb_patches, kernel_size=patch_size, stride=patch_size)\n",
    "print(patchenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch module that takes an image and returns a sequence of patches where the size of each patch is 4*4 and the output\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size=4, in_channels=3, embed_dim=64, img_size=32, nb_patches=0):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        if nb_patches == 0:\n",
    "            self.num_patches = (img_size // patch_size) ** 2\n",
    "        else:\n",
    "            self.nb_patches = nb_patches\n",
    "        # Conv2d replaces manually extracting patches\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=self.in_channels, \n",
    "            out_channels=self.embed_dim, \n",
    "            kernel_size=self.patch_size, \n",
    "            stride=self.patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)\n",
    "        #permute to have the batch size first\n",
    "        x = x.permute(0, 2, 1) \n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "Image Shape with Batch: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "#take one image as tensor from train loader\n",
    "image, _ = trainset[0]\n",
    "print(image.shape)\n",
    "\n",
    "patch_size = 4\n",
    "\n",
    "color_channels = patch_size**2\n",
    "\n",
    "# calculate the number of patches for the image\n",
    "nb_patches = height * width // patch_size**2\n",
    "\n",
    "#patch_embedding = PatchEmbedding(patch_size=patch_size, \n",
    "#                                 in_channels=color_channels, \n",
    "#                                 nb_patch=16)\n",
    "#patches = patch_embedding(image)\n",
    "\n",
    "patch_embed = PatchEmbedding(patch_size=4, in_channels=3, embed_dim=64, img_size=32, nb_patches=nb_patches)\n",
    "\n",
    "image = image.unsqueeze(0)\n",
    "print(\"Image Shape with Batch:\", image.shape)\n",
    "\n",
    "patches = patch_embed(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "    d_k = q.size()[-1]\n",
    "    #multiplication matricielle entre q et k\n",
    "    attn_logits = q @ k.transpose(-2, -1)\n",
    "    #scaling avec d_k (voir equation)\n",
    "    attn_logits *= (1.0 / math.sqrt(d_k))\n",
    "    #faire le softmax sur la dernière dimension\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    #multiplication matricielle avec v\n",
    "    values = attention @ v\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " torch.Size([3, 2])\n",
      "K\n",
      " torch.Size([3, 2])\n",
      "V\n",
      " torch.Size([3, 2])\n",
      "Values\n",
      " torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q.shape)\n",
    "print(\"K\\n\", k.shape)\n",
    "print(\"V\\n\", v.shape)\n",
    "print(\"Values\\n\", values.shape)\n",
    "#doit correspondre à la même shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do q k v projection with 3 nn.linear instead of just one mlp with 3 outputs (q, k, v heads)\n",
    "class dumb_qkv_proj(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.q = nn.Linear(self.d_in, self.d_out)\n",
    "        self.k = nn.Linear(self.d_in, self.d_out)\n",
    "        self.v = nn.Linear(self.d_in, self.d_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qkv_proj(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.proj = nn.Linear(self.d_in, self.d_out * 3)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        return x.chunk(3, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Q\n",
      " torch.Size([3, 2])\n",
      "K\n",
      " torch.Size([3, 2])\n",
      "V\n",
      " torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "proj = qkv_proj(d_in=2, d_out=2)\n",
    "res = proj(values)\n",
    "print(len(res))\n",
    "q, k, v = res\n",
    "print(\"Q\\n\", q.shape)\n",
    "print(\"K\\n\", k.shape)\n",
    "print(\"V\\n\", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qkv_proj(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.proj = nn.Linear(self.d_in, self.d_out * 3)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        qkv = self.proj(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, -1)\n",
    "        #permute Batch, Head, SeqLen, Dims\n",
    "        qkv = qkv.permute(0, 2, 1, 3) \n",
    "        return qkv.chunk(3, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 64, 24]),\n",
       " torch.Size([1, 4, 64, 24]),\n",
       " torch.Size([1, 4, 64, 24]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q,k,v = qkv_proj(32, 32*3, 4)(torch.randn(1,64,32))\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        #ps: juste pour vérifier que votre dimension\n",
    "        #   match bien le nombre de tête...\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        #in_proj - votre fonction qkv_proj créé précédement\n",
    "        self.in_proj = qkv_proj(input_dim, embed_dim, num_heads)\n",
    "        \n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        q, k, v = self.in_proj(x)\n",
    "        #print(q.shape, k.shape, v.shape)\n",
    "        #appeler votre fonction scaled_dot_product\n",
    "        attn = scaled_dot_product(q, k, v)\n",
    "        #print(attn.shape)\n",
    "        #Permute back\n",
    "        #Votre position de départ : [Batch, Head, SeqLen, Dims]\n",
    "        #Permute dans la nouvelle dim : [Batch, SeqLen, Head, Dims]\n",
    "        #(B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        attn = attn.permute(0, 2, 1, 3)\n",
    "        #print(attn.shape)\n",
    "        \n",
    "        #reshape pour retirer la dimension \"head\".\n",
    "        #aide : position de départ [Batch, SeqLen, Head, Dims]\n",
    "        # position d'arrivée [Batch, SeqLen, self.embed_dim]\n",
    "        attn = attn.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        #print(attn.shape)\n",
    "        out = self.o_proj(attn)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MultiheadAttention(64, 64, 4)\n",
    "net(torch.randn(1,16*16,64)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MultiheadAttention(64, 256, 4)\n",
    "net(torch.randn(1,16*16,64)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-head Attention\n",
    "        self.attention = MultiheadAttention(input_dim=input_dim, embed_dim=input_dim, num_heads=num_heads)\n",
    "        self.norm1 = nn.LayerNorm(input_dim)  # Normalisation après l'attention\n",
    "\n",
    "        # Feed Forward\n",
    "        self.ff = FeedForward(input_dim, hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)  # Normalisation après le feed forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head Attention\n",
    "        attn_out = self.attention(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "\n",
    "        # Feed Forward\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 128])\n",
      "image: torch.Size([1, 3, 32, 32])\n",
      "pacthes: torch.Size([1, 64, 64])\n",
      "torch.Size([1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'entrée : séquence de patches (batch_size=1, num_patches=64, embed_dim=128)\n",
    "x = torch.randn(1, 64, 128)\n",
    "\n",
    "# Définition du Transformer Block\n",
    "transformer = Transformer(input_dim=128, num_heads=8, hidden_dim=256)\n",
    "\n",
    "# Passage avant\n",
    "output = transformer(x)\n",
    "print(output.shape)\n",
    "\n",
    "print(\"image:\", image.shape)\n",
    "patches = patch_embed(image)\n",
    "print(\"pacthes:\", patches.shape)\n",
    "transformer = Transformer(input_dim=64, num_heads=8, hidden_dim=256)\n",
    "output = transformer(patches)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TowerViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, hidden_dim, num_transformers, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(patch_size=patch_size, \n",
    "                                              in_channels=in_channels, \n",
    "                                              nb_patches=((image_size*image_size) // patch_size) ** 2)\n",
    "\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "\n",
    "        #stacking n number of transformers into a single sequential\n",
    "        self.transformers = nn.Sequential(*[Transformer(embed_dim, num_heads, hidden_dim) for _ in range(num_transformers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = self.patch_embedding(x)\n",
    "        \n",
    "        # Ajout du token de classification\n",
    "        cls_tokens = self.class_token.expand(batch_size, -1, -1)  # (batch, 1, embed_dim)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (batch, num_patches + 1, embed_dim)\n",
    "        \n",
    "        # Ajout de l'embedding positionnel\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        x = self.transformers(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Classification (on récupère seulement le token [CLS])\n",
    "        cls_out = x[:, 0]\n",
    "        return self.mlp_head(cls_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ViT model was itself created as a combination of the specifications of the instructions for this project as well as insights I was able to gather from the paper which introduced vision transformers (\"AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\", https://arxiv.org/abs/2010.11929).  \n",
    "The architecture in the paper which reflects parts of this implementation are as follows :\n",
    "![vit_arch](vit_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, in_embed_dims, out_embed_dims, num_heads, hidden_dims, num_transformers, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(in_embed_dims) == num_transformers, \"Mismatch: in_embed_dims length must match num_transformers\"\n",
    "        assert len(out_embed_dims) == num_transformers, \"Mismatch: out_embed_dims length must match num_transformers\"\n",
    "        assert len(hidden_dims) == num_transformers, \"Mismatch: hidden_dims length must match num_transformers\"\n",
    "        \n",
    "        num_patches = ((image_size*image_size) // patch_size) ** 2\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, in_embed_dims[0]))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, in_embed_dims[0]))\n",
    "\n",
    "        # Patch Embedding\n",
    "        self.patch_embedding = PatchEmbedding(patch_size=patch_size, \n",
    "                                              in_channels=in_channels, \n",
    "                                              nb_patches=num_patches)\n",
    "\n",
    "        self.transformers = nn.ModuleList()\n",
    "        for i in range(num_transformers):\n",
    "            transformer = Transformer(in_embed_dims[i], num_heads[i], hidden_dims[i])\n",
    "            self.transformers.append(transformer)\n",
    "\n",
    "            if in_embed_dims[i] != out_embed_dims[i]:  # Changement de dimension si nécessaire\n",
    "                self.transformers.append(nn.Linear(in_embed_dims[i], out_embed_dims[i]))\n",
    "\n",
    "        last_embed_dim = out_embed_dims[-1]\n",
    "\n",
    "        self.norm = nn.LayerNorm(last_embed_dim)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(last_embed_dim, hidden_dims[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[-1], num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width, height = x.shape\n",
    "\n",
    "        # Patch Embedding\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # Ajout du token de classification\n",
    "        cls_tokens = self.class_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        #x += self.pos_embedding\n",
    "\n",
    "        for layer in self.transformers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Classification (on prend le token CLS)\n",
    "        cls_out = x[:, 0]\n",
    "        return self.mlp_head(cls_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "patch_size = 4\n",
    "in_channels = 3\n",
    "\n",
    "embed_dim = 64\n",
    "hidden_dim = 128\n",
    "\n",
    "n_heads = 4\n",
    "num_transformers = 3\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "towervit = TowerViT(image_size, patch_size, in_channels, embed_dim, n_heads, hidden_dim, num_transformers, num_classes)\n",
    "\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "output = towervit(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "TowerViT                                 [1, 10]                   4,224\n",
      "├─PatchEmbedding: 1-1                    [1, 64, 64]               --\n",
      "│    └─Conv2d: 2-1                       [1, 64, 8, 8]             3,136\n",
      "├─Sequential: 1-2                        [1, 65, 64]               --\n",
      "│    └─Transformer: 2-2                  [1, 65, 64]               --\n",
      "│    │    └─MultiheadAttention: 3-1      [1, 65, 64]               16,640\n",
      "│    │    └─LayerNorm: 3-2               [1, 65, 64]               128\n",
      "│    │    └─FeedForward: 3-3             [1, 65, 64]               16,576\n",
      "│    │    └─LayerNorm: 3-4               [1, 65, 64]               128\n",
      "│    └─Transformer: 2-3                  [1, 65, 64]               --\n",
      "│    │    └─MultiheadAttention: 3-5      [1, 65, 64]               16,640\n",
      "│    │    └─LayerNorm: 3-6               [1, 65, 64]               128\n",
      "│    │    └─FeedForward: 3-7             [1, 65, 64]               16,576\n",
      "│    │    └─LayerNorm: 3-8               [1, 65, 64]               128\n",
      "│    └─Transformer: 2-4                  [1, 65, 64]               --\n",
      "│    │    └─MultiheadAttention: 3-9      [1, 65, 64]               16,640\n",
      "│    │    └─LayerNorm: 3-10              [1, 65, 64]               128\n",
      "│    │    └─FeedForward: 3-11            [1, 65, 64]               16,576\n",
      "│    │    └─LayerNorm: 3-12              [1, 65, 64]               128\n",
      "├─LayerNorm: 1-3                         [1, 65, 64]               128\n",
      "├─Sequential: 1-4                        [1, 10]                   --\n",
      "│    └─Linear: 2-5                       [1, 128]                  8,320\n",
      "│    └─GELU: 2-6                         [1, 128]                  --\n",
      "│    └─Linear: 2-7                       [1, 10]                   1,290\n",
      "==========================================================================================\n",
      "Total params: 117,514\n",
      "Trainable params: 117,514\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.31\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.97\n",
      "Params size (MB): 0.45\n",
      "Estimated Total Size (MB): 1.43\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(towervit, (1, in_channels, image_size, image_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 391/391 [00:07<00:00, 50.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 910, Total: 10000, Accuracy: 0.09\n",
      "Accuracy: 9.1% (0.091)\n",
      "Loss: 2.3027187774055884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 79/79 [00:01<00:00, 76.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 2.3027187774055884 valid 2.2730846405029297\n",
      "Saving model\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 391/391 [00:06<00:00, 60.36it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns/towerVIT_trainer_validation_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(timestamp))\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtraining_and_validation_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtowervit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chps0906/transformers/utils.py:92\u001b[0m, in \u001b[0;36mtraining_and_validation_loop\u001b[0;34m(model, train_data, test_data, epochs, writer, device, opt, timestamp, flatten, move_batch)\u001b[0m\n\u001b[1;32m     90\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     91\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m fit_one_cycle(model, train_data, opt, epoch, writer, device, flatten)\n\u001b[0;32m---> 92\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/chps0906/transformers/utils.py:17\u001b[0m, in \u001b[0;36mprediction_accuracy\u001b[0;34m(model, data, device, flatten)\u001b[0m\n\u001b[1;32m     15\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, label \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 17\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device), \u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flatten:\n\u001b[1;32m     20\u001b[0m         image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(image, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m32\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m32\u001b[39m))  \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "epochs = 200\n",
    "learning_rate = 3e-4\n",
    "towervit = towervit.to(device)\n",
    "\n",
    "#using the same optimizer as in the ViT paper\n",
    "opt = torch.optim.SGD(towervit.parameters(), lr=learning_rate, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/towerVIT_trainer_validation_{}'.format(timestamp))\n",
    "training_and_validation_loop(towervit, train_loader, test_loader, epochs, writer, device, opt, timestamp, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "patch_size = 4\n",
    "in_channels = 3\n",
    "\n",
    "in_embed_dims = [64, 128, 256]  \n",
    "out_embed_dims = [128, 256, 256]\n",
    "num_heads = [4, 8, 16]\n",
    "hidden_dims = [128, 256, 512]\n",
    "num_transformers = 3\n",
    "num_classes = 10\n",
    "\n",
    "vit = ViT(image_size, patch_size, in_channels, in_embed_dims, out_embed_dims, num_heads, hidden_dims, num_transformers, num_classes)\n",
    "\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "output = vit(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ViT                                      [1, 10]                   4,194,432\n",
      "├─PatchEmbedding: 1-1                    [1, 64, 64]               --\n",
      "│    └─Conv2d: 2-1                       [1, 64, 8, 8]             3,136\n",
      "├─ModuleList: 1-2                        --                        --\n",
      "│    └─Transformer: 2-2                  [1, 65, 64]               --\n",
      "│    │    └─MultiheadAttention: 3-1      [1, 65, 64]               16,640\n",
      "│    │    └─LayerNorm: 3-2               [1, 65, 64]               128\n",
      "│    │    └─FeedForward: 3-3             [1, 65, 64]               16,576\n",
      "│    │    └─LayerNorm: 3-4               [1, 65, 64]               128\n",
      "│    └─Linear: 2-3                       [1, 65, 128]              8,320\n",
      "│    └─Transformer: 2-4                  [1, 65, 128]              --\n",
      "│    │    └─MultiheadAttention: 3-5      [1, 65, 128]              66,048\n",
      "│    │    └─LayerNorm: 3-6               [1, 65, 128]              256\n",
      "│    │    └─FeedForward: 3-7             [1, 65, 128]              65,920\n",
      "│    │    └─LayerNorm: 3-8               [1, 65, 128]              256\n",
      "│    └─Linear: 2-5                       [1, 65, 256]              33,024\n",
      "│    └─Transformer: 2-6                  [1, 65, 256]              --\n",
      "│    │    └─MultiheadAttention: 3-9      [1, 65, 256]              263,168\n",
      "│    │    └─LayerNorm: 3-10              [1, 65, 256]              512\n",
      "│    │    └─FeedForward: 3-11            [1, 65, 256]              262,912\n",
      "│    │    └─LayerNorm: 3-12              [1, 65, 256]              512\n",
      "├─LayerNorm: 1-3                         [1, 65, 256]              512\n",
      "├─Sequential: 1-4                        [1, 10]                   --\n",
      "│    └─Linear: 2-7                       [1, 512]                  131,584\n",
      "│    └─ReLU: 2-8                         [1, 512]                  --\n",
      "│    └─Linear: 2-9                       [1, 10]                   5,130\n",
      "==========================================================================================\n",
      "Total params: 5,069,194\n",
      "Trainable params: 5,069,194\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 1.07\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.47\n",
      "Params size (MB): 3.50\n",
      "Estimated Total Size (MB): 5.98\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(vit, (1, in_channels, image_size, image_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "epochs = 200\n",
    "learning_rate = 3e-4\n",
    "vit = vit.to(device)\n",
    "\n",
    "#using the same optimizer as in the ViT paper\n",
    "opt = torch.optim.SGD(vit.parameters(), lr=learning_rate, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/VIT_trainer_validation_{}'.format(timestamp))\n",
    "training_and_validation_loop(vit, train_loader, test_loader, epochs, writer, device, opt, timestamp, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
